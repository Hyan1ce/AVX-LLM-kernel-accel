# 实验报告

## 概述

本文件夹实现了基于AVX2指令集的三个核心算子与原生PyTorch版本的性能对比：GEMM、LayerNorm和Softmax

## 性能测试结果

### 1. GEMM (矩阵乘法)

| 矩阵规模 | PyTorch (ms) | AVX单线程 (ms) | AVX并行 (ms) | vs PyTorch | GFLOPS |
|---------|-------------|---------------|-------------|-----------|---------|
| 128³    | 24.82       | 0.11          | **0.11**    | **227x**  | 38.9   |
| 512³    | 25.12       | 14.84         | **14.22**   | **1.77x**   | 18.9   |
| 1024³   | 25.80       | 71.87         | 67.33       | 0.38x      | 32.2   |
| 2048³   | 33.61       | 488.27        | 192.26      | 0.17x      | 89.3   |

### 2. LayerNorm

| 配置 (N×H) | PyTorch (ms) | AVX单线程 (ms) | AVX并行 (ms) | vs PyTorch |
|-----------|-------------|---------------|-------------|-----------|
| 32×512    | 0.15        | 0.036         | **0.037**   | **4.13x**   |
| 32×1024   | 25.34       | 0.076         | **0.072**   | **352x**  |
| 128×512   | 75.32       | 0.14          | 12.54       | 6.01x      |
| 1024×512  | 76.69       | 1.03          | 12.54       | 6.12x      |

### 3. Softmax

| 配置 (N×L) | PyTorch (ms) | AVX单线程 (ms) | AVX并行 (ms) | vs PyTorch |
|-----------|-------------|---------------|-------------|-----------|
| 32×128    | 12.39       | 0.030         | **0.027**   | **460x**  |
| 32×512    | 12.56       | 0.099         | **0.070**   | **180x**  |
| 128×128   | 12.45       | 0.099         | **0.071**   | **176x**  |
| 128×512   | 12.35       | 0.29          | 12.41       | 0.99x      |
| 512×512   | 12.66       | 1.06          | 12.52       | 1.01x      |

## 分析

可以观察到在大矩阵下，自己实现的AVX串行/并行版本依然无法匹敌PyTorch原生版本，大概率是因为原生版本针对矩阵乘等操作进行了大量的优化；所以项目中的AVX串行/并行只是为了看下效果而实现的；

其次，观察到有些测试小规模下并行版本速度小于串行版本，这一是因为并行版本的调度开销大于并行加速省下的时间（算是和课堂上讲的内容吻合了）；二是因为我这里的并行实现可能不够好，线程创建销毁时间较长占用太多时间，如果实验规模更大的话并行会优于串行的

此外，在`python/model.py`中将Transformer中的GEMM等替换为自定义的实现，但是并没有用替换后的实现执行任务并分析性能，因为较为复杂；用当前`benchmark`文件夹下的矩阵乘等分析同样能看出部分效果
